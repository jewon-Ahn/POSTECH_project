{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Module import 및 환경 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "#from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input data 경로설정\n",
    "curr_dir = os.getcwd()\n",
    "print(curr_dir)\n",
    "\n",
    "os.chdir(\"/home/sookyoung.kim/ssd/projects/GraphDB/customer_index/complaint_index/w12_1-12_1_at_once/test_vectors_gt_cx2_tra\")\n",
    "#os.system('ls')\n",
    "train_data_path = os.getcwd()\n",
    "print(train_data_path)\n",
    "os.chdir(curr_dir)\n",
    "\n",
    "os.chdir(\"/home/sookyoung.kim/ssd/projects/GraphDB/customer_index/complaint_index/w12_1-12_1_at_once/test_vectors_gt_cx2_inf\")\n",
    "#os.system('ls')\n",
    "infer_data_path = os.getcwd()\n",
    "print(infer_data_path)\n",
    "os.chdir(curr_dir)\n",
    "\n",
    "## Input file 명 설정\n",
    "train_data = os.path.join(train_data_path, \"vectors_CUST_NO.csv\")\n",
    "print(train_data)\n",
    "\n",
    "## Input file 명 설정\n",
    "infer_data = os.path.join(infer_data_path, \"vectors_CUST_NO.csv\")\n",
    "print(infer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression을 위한 입력 data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "#from ast import literal_eval\n",
    "\n",
    "vdim = 256\n",
    "vcol_name = 'embedding_'+str(vdim)\n",
    "print(vcol_name)\n",
    "\n",
    "df_train = pd.read_csv(train_data)\n",
    "list_index = df_train.index\n",
    "\n",
    "list_dimensions = []\n",
    "for i in range(0, vdim):\n",
    "    el = 'D'+str(i).zfill(3)\n",
    "    list_dimensions.append(el)\n",
    "\n",
    "# Each vector is stored in string. Convert each vector to list first.\n",
    "series_vectors = df_train[vcol_name].apply(lambda x: ast.literal_eval(x))\n",
    "# Then, make the vectors 2D list\n",
    "list_vectors = series_vectors.to_list()\n",
    "\n",
    "df_vectors = pd.DataFrame(data=list_vectors, index=list_index, columns=list_dimensions)\n",
    "\n",
    "df_train.drop([vcol_name], axis=1, inplace=True)\n",
    "\n",
    "df_vectors['CUST_NO'] = df_train['CUST_NO']\n",
    "\n",
    "df_train = pd.merge(df_train, df_vectors, how='left', on=['CUST_NO'])\n",
    "\n",
    "df_train.to_csv('df_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "#from ast import literal_eval\n",
    "\n",
    "vdim = 256\n",
    "vcol_name = 'embedding_'+str(vdim)\n",
    "print(vcol_name)\n",
    "\n",
    "df_infer = pd.read_csv(infer_data)\n",
    "list_index = df_infer.index\n",
    "\n",
    "list_dimensions = []\n",
    "for i in range(0, vdim):\n",
    "    el = 'D'+str(i).zfill(3)\n",
    "    list_dimensions.append(el)\n",
    "\n",
    "# Each vector is stored in string. Convert each vector to list first.\n",
    "series_vectors = df_infer[vcol_name].apply(lambda x: ast.literal_eval(x))\n",
    "# Then, make the vectors 2D list\n",
    "list_vectors = series_vectors.to_list()\n",
    "\n",
    "df_vectors = pd.DataFrame(data=list_vectors, index=list_index, columns=list_dimensions)\n",
    "\n",
    "df_infer.drop([vcol_name], axis=1, inplace=True)\n",
    "\n",
    "df_vectors['CUST_NO'] = df_infer['CUST_NO']\n",
    "\n",
    "df_infer = pd.merge(df_infer, df_vectors, how='left', on=['CUST_NO'])\n",
    "\n",
    "df_infer.to_csv('df_infer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_infer = pd.concat([df_train, df_infer], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_infer 에서  \n",
    "# df_train_noclaim\n",
    "# df_train_claim\n",
    "# df_infer_noclaim\n",
    "# df_infer_claim\n",
    "# df_train_infer_noclaim\n",
    "# df_train_infer_claim\n",
    "# 으로 나눔\n",
    "\n",
    "cond_tmp = ((df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 0) | (df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 2)) & \\\n",
    "            (df_train_infer.loc[:, 'LABEL_CLAIM'] == 0)\n",
    "df_train_noclaim = df_train_infer.loc[cond_tmp, :]\n",
    "cond_tmp = ((df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 0) | (df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 2)) & \\\n",
    "            (df_train_infer.loc[:, 'LABEL_CLAIM'] == 1)\n",
    "df_train_claim = df_train_infer.loc[cond_tmp, :]\n",
    "\n",
    "cond_tmp = ((df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 1) | (df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 2)) & \\\n",
    "            (df_train_infer.loc[:, 'LABEL_CLAIM'] == 0)\n",
    "df_infer_noclaim = df_train_infer.loc[cond_tmp, :]\n",
    "cond_tmp = ((df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 1) | (df_train_infer.loc[:, 'FLAG_TRAIN_INFER'] == 2)) & \\\n",
    "            (df_train_infer.loc[:, 'LABEL_CLAIM'] == 1)\n",
    "df_infer_claim = df_train_infer.loc[cond_tmp, :]\n",
    "\n",
    "cond_tmp = (df_train_infer.loc[:, 'LABEL_CLAIM'] == 0)\n",
    "df_train_infer_noclaim = df_train_infer.loc[cond_tmp, :]\n",
    "\n",
    "cond_tmp = (df_train_infer.loc[:, 'LABEL_CLAIM'] == 1)\n",
    "df_train_infer_claim = df_train_infer.loc[cond_tmp, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_infer_claim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train set과 inference set 간 claim/noclaim 샘플들 간 유사도 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1시간 이상 걸립니다. Plotting해 상태를 보기 위함이니 생략해도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "# UMAP dimension reduction from 256 to 2\n",
    "def umap_dimension_reduction_d256_to_d2(df_h, n_neighbors, min_dist, metric, output_df_col_list):\n",
    "        \n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, metric=metric, verbose=True)\n",
    "       \n",
    "    list_header = []\n",
    "    for i in range(0, 256):\n",
    "        el = 'D'+str(i).zfill(3)\n",
    "        list_header.append(el)\n",
    "    \n",
    "    hd_vector_data = df_h[list_header].values\n",
    "    ld_vector_data = reducer.fit_transform(hd_vector_data)\n",
    "    \n",
    "    df_l = pd.DataFrame(data = ld_vector_data.tolist(), columns = output_df_col_list, index=df_h.index)\n",
    "    \n",
    "    return df_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df_train_infer_claim_vector_ld, \n",
    "                x='D1', y='D2', \n",
    "                hue='FLAG_TRAIN_INFER', palette='muted').set_title('Train Vs. Inference clusters');\n",
    "plt.ylim(-10, 20)\n",
    "plt.xlim(-4, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP을 이용한 graph embedded vectors (256차원)의 차원 축소 (2차원)\n",
    "df_train_infer_noclaim_vector = df_train_infer_noclaim.copy()\n",
    "df_train_infer_noclaim_vector.drop(['CUST_NO'], axis=1, inplace=True)\n",
    "df_train_infer_noclaim_vector.drop(['CX_TYPE'], axis=1, inplace=True)\n",
    "df_train_infer_noclaim_vector.drop(['FLAG_TRAIN_INFER'], axis=1, inplace=True)\n",
    "df_train_infer_noclaim_vector.drop(['LABEL_CLAIM'], axis=1, inplace=True)\n",
    "df_train_infer_noclaim_vector.drop(['LABEL_CLAIM_INF'], axis=1, inplace=True)\n",
    "df_train_infer_noclaim_vector.drop(['LABEL_CLAIM_FUTURE'], axis=1, inplace=True)\n",
    "df_train_infer_noclaim_vector.drop(['LABEL_CLAIM_NEXT'], axis=1, inplace=True)\n",
    "df_train_infer_noclaim_vector.drop(['RCPT_DTHM'], axis=1, inplace=True)\n",
    "\n",
    "df_train_infer_noclaim_vector_ld = umap_dimension_reduction_d256_to_d2(df_train_infer_noclaim_vector, 50, 0.1, 'euclidean', ['D1','D2'])\n",
    "\n",
    "df_train_infer_noclaim_vector_ld['FLAG_TRAIN_INFER'] = df_train_infer_noclaim['FLAG_TRAIN_INFER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df_train_infer_noclaim_vector_ld, \n",
    "                x='D1', y='D2', \n",
    "                hue='FLAG_TRAIN_INFER', palette='muted').set_title('Train Vs. Inference clusters');\n",
    "plt.ylim(-10, 20)\n",
    "plt.xlim(-4, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distance-based down sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 vector들의 분포 양상을 확인한 결과, label 1과 0인 sample들이 겹쳤기에, train set으로 학습해도 inference set을 제대로 예측하지 못할 가능성이 높습니다.\n",
    "\n",
    "따라서, train set의 label 1인 sample들과 의도적으로 거리가 먼 label 0 sample들만 추출해 학습시키는 negative sampling을 하는 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set 내의 claim samples의 자신들의 centroid로부터의 거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "import math\n",
    "\n",
    "# Claim samples의 centroid 찾기\n",
    "list_train_claim_centroid = df_train_claim[list_dimensions].mean().to_list()\n",
    "train_claim_centroid = np.array(list_train_claim_centroid)\n",
    "#print(list_train_claim_centroid)\n",
    "#print(sum(list_train_claim_centroid) / len(list_train_claim_centroid))\n",
    "\n",
    "# Noclaim samples와의 거리 계산\n",
    "df_train_claim_dist = df_train_claim[list_dimensions]\n",
    "df_train_claim_dist['DIST'] = df_train_claim_dist.sub(train_claim_centroid, axis=1).pow(2).sum(axis=1).pow(.5)\n",
    "\n",
    "train_claim_dist_max = df_train_claim_dist['DIST'].max()\n",
    "train_claim_dist_min = df_train_claim_dist['DIST'].min()\n",
    "print(train_claim_dist_max, train_claim_dist_min)\n",
    "\n",
    "df_train_claim_dist['CUST_NO'] = df_train_claim['CUST_NO']\n",
    "df_train_claim_dist['CX_TYPE'] = df_train_claim['CX_TYPE']\n",
    "df_train_claim_dist['FLAG_TRAIN_INFER'] = df_train_claim['FLAG_TRAIN_INFER']\n",
    "df_train_claim_dist['LABEL_CLAIM'] = df_train_claim['LABEL_CLAIM']\n",
    "df_train_claim_dist['LABEL_CLAIM_INF'] = df_train_claim['LABEL_CLAIM_INF']\n",
    "df_train_claim_dist['LABEL_CLAIM_FUTURE'] = df_train_claim['LABEL_CLAIM_FUTURE']\n",
    "df_train_claim_dist['LABEL_CLAIM_NEXT'] = df_train_claim['LABEL_CLAIM_NEXT']\n",
    "df_train_claim_dist['RCPT_DTHM'] = df_train_claim['RCPT_DTHM']\n",
    "\n",
    "#df_train_claim_dist = df_train_claim_dist.sort_values(by=['DIST'], ascending=False)\n",
    "df_train_claim_dist = df_train_claim_dist.sort_values(by=['DIST'], ascending=True)\n",
    "print(df_train_claim_dist['DIST'])\n",
    "df_train_claim_dist.reset_index(drop=True, inplace=True)\n",
    "#df_train_claim_dist.drop(['DIST'], axis=1, inplace=True)\n",
    "\n",
    "# Down sampling noclaim\n",
    "len_train_claim = len(df_train_claim)\n",
    "print(len_train_claim)\n",
    "\n",
    "'''\n",
    "df_train_claim_undersamples = df_train_claim_dist.iloc[0:len_train_claim*3,:]\n",
    "#print(df_train_noclaim_undersamples)\n",
    "\n",
    "df_train_claim_undersamples.drop(['DIST'], axis=1, inplace=True)\n",
    "\n",
    "# Make up the train set\n",
    "df_train_undersamples = pd.concat([df_train_claim, df_train_claim_undersamples], ignore_index=True)\n",
    "#print(df_train_claim_undersamples)\n",
    "#print(df_train_undersamples)\n",
    "#df_train_undersamples.to_csv('vectors_CUST_NO_under.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_noclaim_centroid = df_train_noclaim[list_dimensions].mean().to_list()\n",
    "train_noclaim_centroid = np.array(list_train_noclaim_centroid)\n",
    "print(list_train_noclaim_centroid)\n",
    "print(sum(list_train_noclaim_centroid) / len(list_train_noclaim_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set 내의 noclaim samples의 claim samples centroid로부터의 거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "### Euclidean distance\n",
    "\n",
    "# Claim samples의 centroid 찾기\n",
    "list_train_claim_centroid = df_train_claim[list_dimensions].mean().to_list()\n",
    "train_claim_centroid = np.array(list_train_claim_centroid)\n",
    "\n",
    "# Noclaim samples와의 거리 계산\n",
    "df_train_noclaim_dist = df_train_noclaim[list_dimensions]\n",
    "df_train_noclaim_dist['DIST'] = df_train_noclaim_dist.sub(train_claim_centroid, axis=1).pow(2).sum(axis=1).pow(.5)\n",
    "\n",
    "df_train_noclaim_dist['CUST_NO'] = df_train_noclaim['CUST_NO']\n",
    "df_train_noclaim_dist['CX_TYPE'] = df_train_noclaim['CX_TYPE']\n",
    "df_train_noclaim_dist['FLAG_TRAIN_INFER'] = df_train_noclaim['FLAG_TRAIN_INFER']\n",
    "df_train_noclaim_dist['LABEL_CLAIM'] = df_train_noclaim['LABEL_CLAIM']\n",
    "df_train_noclaim_dist['LABEL_CLAIM_INF'] = df_train_noclaim['LABEL_CLAIM_INF']\n",
    "df_train_noclaim_dist['LABEL_CLAIM_FUTURE'] = df_train_noclaim['LABEL_CLAIM_FUTURE']\n",
    "df_train_noclaim_dist['LABEL_CLAIM_NEXT'] = df_train_noclaim['LABEL_CLAIM_NEXT']\n",
    "df_train_noclaim_dist['RCPT_DTHM'] = df_train_noclaim['RCPT_DTHM']\n",
    "\n",
    "df_train_noclaim_dist_gt = df_train_noclaim_dist.sort_values(by=['DIST'], ascending=False)\n",
    "df_train_noclaim_dist_lt = df_train_noclaim_dist.sort_values(by=['DIST'], ascending=True)\n",
    "print(df_train_noclaim_dist_gt['DIST'])\n",
    "df_train_noclaim_dist_gt.reset_index(drop=True, inplace=True)\n",
    "print(df_train_noclaim_dist_lt['DIST'])\n",
    "df_train_noclaim_dist_lt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Down sampling noclaim\n",
    "len_train_claim = len(df_train_claim)\n",
    "print(len_train_claim)\n",
    "\n",
    "if 1:\n",
    "    df_train_noclaim_undersamples_lt = df_train_noclaim_dist_lt.iloc[0:round(len_train_claim*2.5),:]\n",
    "    df_train_noclaim_undersamples_gt = df_train_noclaim_dist_gt.iloc[0:round(len_train_claim*2.5),:]\n",
    "else:\n",
    "    cond_tmp = df_train_noclaim_dist.loc[:, 'DIST'] < train_claim_dist_min\n",
    "    df_train_noclaim_undersamples_lt = \\\n",
    "        df_train_noclaim_dist.loc[cond_tmp, :].sort_values(by=['DIST'], ascending=False).iloc[0:round(len_train_claim*1.5),:]\n",
    "    cond_tmp = df_train_noclaim_dist.loc[:, 'DIST'] > train_claim_dist_max\n",
    "    df_train_noclaim_undersamples_gt = \\\n",
    "        df_train_noclaim_dist.loc[cond_tmp, :].sort_values(by=['DIST'], ascending=True).iloc[0:round(len_train_claim*1.5),:]\n",
    "\n",
    "df_train_noclaim_undersamples = pd.concat([df_train_noclaim_undersamples_lt, df_train_noclaim_undersamples_gt], ignore_index=True)\n",
    "\n",
    "df_train_noclaim_undersamples.drop(['DIST'], axis=1, inplace=True)\n",
    "\n",
    "# Make up the train set\n",
    "df_train_undersamples = pd.concat([df_train_claim, df_train_noclaim_undersamples], ignore_index=True)\n",
    "#print(df_train_noclaim_undersamples)\n",
    "#print(df_train_undersamples)\n",
    "df_train_undersamples.to_csv('vectors_CUST_NO_under.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LGBM regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_undersamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#x = df_train[list_dimensions]\n",
    "#y = df_train[['LABEL_CLAIM']]\n",
    "x = df_train_undersamples[list_dimensions]\n",
    "y = df_train_undersamples[['LABEL_CLAIM']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.99, test_size=0.01)\n",
    "\n",
    "# Ensemble - Boosting - Random forest classification\n",
    "lgbm_train_ds = lgbm.Dataset(x_train, label=y_train)\n",
    "lgbm_test_ds = lgbm.Dataset(x_test, label=y_test)\n",
    "#'''\n",
    "lgbm_params = {\n",
    "    'verbose': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 12,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'mae',\n",
    "    'boosting': 'gbdt',\n",
    "}\n",
    "#'''\n",
    "'''\n",
    "lgbm_params = {\n",
    "    'verbose': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 31,\n",
    "    'num_leaves': 31,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting': 'gbdt',\n",
    "}\n",
    "'''\n",
    "\n",
    "lgbm_model = lgbm.train(params=lgbm_params,\n",
    "                        train_set=lgbm_train_ds,\n",
    "                        #num_boost_round=10000,\n",
    "                        num_boost_round=5000,\n",
    "                        valid_sets=lgbm_test_ds,\n",
    "                        #verbose_eval=False,  # True, False, or a number\n",
    "                        #early_stopping_rounds=1000,\n",
    "                        )\n",
    "\n",
    "df_fit = df_infer.loc[:, list_dimensions]\n",
    "\n",
    "df_fit.loc[:, 'LABEL_CLAIM_PRED'] = lgbm_model.predict(df_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_fit['LABEL_CLAIM'] = df_infer.loc[:, 'LABEL_CLAIM']\n",
    "\n",
    "y_true = df_fit['LABEL_CLAIM'].to_list()\n",
    "#print(y_true)\n",
    "y_pred = df_fit['LABEL_CLAIM_PRED'].to_list()\n",
    "print(y_pred)\n",
    "\n",
    "df_fit.loc[:, 'LABEL_CLAIM_PRED_ROUND'] = 0\n",
    "cond_tmp = df_fit.loc[:, 'LABEL_CLAIM_PRED'] >= 0.5\n",
    "df_fit.loc[cond_tmp, 'LABEL_CLAIM_PRED_ROUND'] = 1\n",
    "y_pred = df_fit['LABEL_CLAIM_PRED_ROUND'].to_list()\n",
    "\n",
    "#print(classification_report(y_true, y_pred, target_names=[0, 1]))\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#print(metrics.f1_score(y_true, y_pred, average='macro'))\n",
    "#print(metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "#print(metrics.f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_fit['LABEL_CLAIM'] = df_infer.loc[:, 'LABEL_CLAIM']\n",
    "\n",
    "y_true = df_fit['LABEL_CLAIM'].to_list()\n",
    "#print(y_true)\n",
    "y_pred = df_fit['LABEL_CLAIM_PRED'].to_list()\n",
    "print(y_pred)\n",
    "\n",
    "df_fit.loc[:, 'LABEL_CLAIM_PRED_ROUND'] = 0\n",
    "cond_tmp = df_fit.loc[:, 'LABEL_CLAIM_PRED'] >= 0.99\n",
    "df_fit.loc[cond_tmp, 'LABEL_CLAIM_PRED_ROUND'] = 1\n",
    "y_pred = df_fit['LABEL_CLAIM_PRED_ROUND'].to_list()\n",
    "\n",
    "#print(classification_report(y_true, y_pred, target_names=[0, 1]))\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#print(metrics.f1_score(y_true, y_pred, average='macro'))\n",
    "#print(metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "#print(metrics.f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
